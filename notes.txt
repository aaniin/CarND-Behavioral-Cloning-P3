LeNet
    Epoch 1/7
    7615/7615 [==============================] - 68s 8ms/step - loss: 0.1616 - val_loss: 0.0015
    Epoch 2/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 0.0023 - val_loss: 0.0088
    Epoch 3/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 0.0032 - val_loss: 0.0021
    Epoch 4/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 0.0013 - val_loss: 0.0025
    Epoch 5/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 0.0012 - val_loss: 0.0022
    Epoch 6/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 9.2966e-04 - val_loss: 0.0019
    Epoch 7/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 8.4268e-04 - val_loss: 0.0022

LeNetDrop
  - default learning rate
    Epoch 1/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 0.0713 - val_loss: 0.0024
    Epoch 2/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0056 - val_loss: 0.0046
    Epoch 3/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0023 - val_loss: 0.0021
    Epoch 4/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0014 - val_loss: 0.0023
    Epoch 5/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0013 - val_loss: 0.0019
    Epoch 6/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0010 - val_loss: 0.0022
    Epoch 7/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0011 - val_loss: 0.0021


LeNetDrop
  - lr 0.001 (default) + 0.96 exp. lr decay w/ 20000 steps
    Epoch 1/7
    7615/7615 [==============================] - 63s 8ms/step - loss: 0.0207 - val_loss: 0.0051
    Epoch 2/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0025 - val_loss: 0.0019
    Epoch 3/7
    7615/7615 [==============================] - 62s 8ms/step - loss: 0.0012 - val_loss: 0.0018
    Epoch 4/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0012 - val_loss: 0.0017
    Epoch 5/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 0.0010 - val_loss: 0.0019
    Epoch 6/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 9.1366e-04 - val_loss: 0.0019
    Epoch 7/7
    7615/7615 [==============================] - 61s 8ms/step - loss: 8.3805e-04 - val_loss: 0.0021

LeNetDrop2
  - two dropout: 0.4 and 0.2
  - lr 0.001 (default) + 0.96 exp. lr decay w/ 20000 steps
    Epoch 1/7
    7615/7615 [==============================] - 64s 8ms/step - loss: 0.0059 - val_loss: 0.0024
    Epoch 2/7
    7615/7615 [==============================] - 62s 8ms/step - loss: 0.0023 - val_loss: 0.0029
    Epoch 3/7
    7615/7615 [==============================] - 62s 8ms/step - loss: 0.0017 - val_loss: 0.0025
    Epoch 4/7
    7615/7615 [==============================] - 62s 8ms/step - loss: 0.0014 - val_loss: 0.0020
    Epoch 5/7
    7615/7615 [==============================] - 62s 8ms/step - loss: 0.0014 - val_loss: 0.0025
    Epoch 6/7
    7615/7615 [==============================] - 62s 8ms/step - loss: 0.0013 - val_loss: 0.0023
    Epoch 7/7
    7615/7615 [==============================] - 62s 8ms/step - loss: 0.0013 - val_loss: 0.0018

There seems to be some data added here?! Or did batch size change??
AlexNet500
  - lr 0.001 (default) + 0.96 exp. lr decay w/ 20000 steps
    Epoch 1/7
    8567/8567 [==============================] - 236s 27ms/step - loss: 0.0180 - val_loss: 0.0115
    Epoch 2/7
    8567/8567 [==============================] - 225s 26ms/step - loss: 0.0088 - val_loss: 0.0048
    Epoch 3/7
    8567/8567 [==============================] - 228s 27ms/step - loss: 0.0091 - val_loss: 0.0100
    Epoch 4/7
    8567/8567 [==============================] - 227s 26ms/step - loss: 0.0091 - val_loss: 0.0042
    Epoch 5/7
    8567/8567 [==============================] - 229s 27ms/step - loss: 0.0094 - val_loss: 0.0044
    Epoch 6/7
    8567/8567 [==============================] - 225s 26ms/step - loss: 0.0096 - val_loss: 0.0057
    Epoch 7/7
    8567/8567 [==============================] - 229s 27ms/step - loss: 0.0097 - val_loss: 0.0043

Nvidia - no dropout
	Epoch 1/7
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0082 - val_loss: 0.0041
	Epoch 2/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 3/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 4/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 5/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 6/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0042
	Epoch 7/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0040

Nvidia - drop 0.4, 0.2
	Epoch 1/7
	8567/8567 [==============================] - 53s 6ms/step - loss: 0.0082 - val_loss: 0.0054
	Epoch 2/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 3/7
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/7
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 5/7
	8567/8567 [==============================] - 51s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 6/7
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 7/7
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0078 - val_loss: 0.0041
LeNetDrop2
  - two dropout: 0.4 and 0.2
  - lr 0.001 (default) + 0.96 exp. lr decay w/ 20000 steps
  - note different train steps - should give the same as above but doesn't.
     * seed?, different dataset, ..?
	Epoch 1/7
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0111 - val_loss: 0.0030
	Epoch 2/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0021 - val_loss: 0.0030
	Epoch 3/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0016 - val_loss: 0.0030
	Epoch 4/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0013 - val_loss: 0.0030
	Epoch 5/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0013 - val_loss: 0.0028
	Epoch 6/7
	8567/8567 [==============================] - 66s 8ms/step - loss: 0.0012 - val_loss: 0.0030
	Epoch 7/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0012 - val_loss: 0.0035
repeat of above (no seed set)
	Epoch 1/7
	8567/8567 [==============================] - 69s 8ms/step - loss: 0.0343 - val_loss: 0.0059
	Epoch 2/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0038 - val_loss: 0.0034
	Epoch 3/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0020 - val_loss: 0.0035
	Epoch 4/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0019 - val_loss: 0.0033
	Epoch 5/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0016 - val_loss: 0.0033
	Epoch 6/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0015 - val_loss: 0.0032
	Epoch 7/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0017 - val_loss: 0.0031
fixed seed 1234 (rest as above)
	Epoch 1/7
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0126 - val_loss: 0.0084
	Epoch 2/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0029 - val_loss: 0.0029
	Epoch 3/7
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0018 - val_loss: 0.0032
	Epoch 4/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0015 - val_loss: 0.0035
	Epoch 5/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0015 - val_loss: 0.0031
	Epoch 6/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0013 - val_loss: 0.0029
	Epoch 7/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0014 - val_loss: 0.0032
repeat with fixed seed 1234
	Epoch 1/7
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0128 - val_loss: 0.0067
	Epoch 2/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0036 - val_loss: 0.0025
	Epoch 3/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0019 - val_loss: 0.0024
	Epoch 4/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0017 - val_loss: 0.0029
	Epoch 5/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0015 - val_loss: 0.0028
	Epoch 6/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0016 - val_loss: 0.0030
	Epoch 7/7
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0014 - val_loss: 0.0029
repeat
	Epoch 7/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0017 - val_loss: 0.0027
max pool (instead of avg)
	Epoch 7/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0021 - val_loss: 0.0032
repeat
	Epoch 7/7
	8567/8567 [==============================] - 67s 8ms/step - loss: 0.0024 - val_loss: 0.0032
--> Avg better than max pool here (4.2M params)
LeNetDrop2XConv
 - avg pool, xtra conv layer - 1.9M params
	Epoch 7/7
	8567/8567 [==============================] - 55s 6ms/step - loss: 0.0013 - val_loss: 0.0026
 - with another xtra conv layer - 550k params
	Epoch 1/7
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0032 - val_loss: 0.0032
	Epoch 2/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0019 - val_loss: 0.0031
	Epoch 3/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0015 - val_loss: 0.0026
	Epoch 4/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0015 - val_loss: 0.0028
	Epoch 5/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0015 - val_loss: 0.0026
	Epoch 6/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0013 - val_loss: 0.0027
	Epoch 7/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0013 - val_loss: 0.0029
 - max pooling in the last conv layer (instead of avg) --> bad idea
	Epoch 1/7
	8567/8567 [==============================] - 51s 6ms/step - loss: 0.0080 - val_loss: 0.0041
	Epoch 2/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 5/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 6/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 7/7
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0078 - val_loss: 0.0041
 - avg pool, train longer - no effect
	Epoch 1/10
	8567/8567 [==============================] - 51s 6ms/step - loss: 0.0032 - val_loss: 0.0030
	Epoch 2/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0017 - val_loss: 0.0028
	Epoch 3/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0015 - val_loss: 0.0023
	Epoch 4/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0014 - val_loss: 0.0027
	Epoch 5/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0014 - val_loss: 0.0023
	Epoch 6/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0014 - val_loss: 0.0023
	Epoch 7/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0014 - val_loss: 0.0021
	Epoch 8/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0014 - val_loss: 0.0038
	Epoch 9/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0014 - val_loss: 0.0024
	Epoch 10/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0015 - val_loss: 0.0023
 - another xtra conv (now 5conv with 1,8,64 after the conv/pool blocks, total 120k params
	Epoch 1/10
	8567/8567 [==============================] - 49s 5ms/step - loss: 0.0080 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 47s 5ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 47s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 46s 5ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 5/10
	8567/8567 [==============================] - 47s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 6/10
	8567/8567 [==============================] - 47s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 7/10
	8567/8567 [==============================] - 47s 6ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 8/10
	8567/8567 [==============================] - 47s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 9/10
	8567/8567 [==============================] - 47s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 10/10
	8567/8567 [==============================] - 46s 5ms/step - loss: 0.0078 - val_loss: 0.0040
 - 5conv but no pooling before dense. 3,16,64 before dense, 430k params
	Epoch 1/10
	8567/8567 [==============================] - 51s 6ms/step - loss: 0.0032 - val_loss: 0.0024
	Epoch 2/10
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0015 - val_loss: 0.0019
	Epoch 3/10
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0015 - val_loss: 0.0025
	Epoch 4/10
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0014 - val_loss: 0.0021
	Epoch 5/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0014 - val_loss: 0.0021
	Epoch 6/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0013 - val_loss: 0.0024
	Epoch 7/10
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 8/10
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0013 - val_loss: 0.0031
	Epoch 9/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0012 - val_loss: 0.0023
	Epoch 10/10
	8567/8567 [==============================] - 49s 6ms/step - loss: 0.0013 - val_loss: 0.0020
	
all above is with 40 cropping form top - 430k params
	 Layer (type)                Output Shape              Param #   
	=================================================================
	 input_1 (InputLayer)        [(None, 160, 320, 3)]     0         
	 cropping2d (Cropping2D)     (None, 120, 320, 3)       0         
	 rescaling (Rescaling)       (None, 120, 320, 3)       0         
	 conv2d (Conv2D)             (None, 120, 320, 6)       456       
	 average_pooling2d (AverageP  (None, 60, 160, 6)       0         
	 ooling2D)                                                       
	 conv2d_1 (Conv2D)           (None, 56, 156, 16)       2416      
	 average_pooling2d_1 (Averag  (None, 28, 78, 16)       0         
	 ePooling2D)                                                     
	 conv2d_2 (Conv2D)           (None, 26, 76, 32)        4640      
	 average_pooling2d_2 (Averag  (None, 13, 38, 32)       0         
	 ePooling2D)                                                     
	 conv2d_3 (Conv2D)           (None, 11, 36, 48)        13872     
	 average_pooling2d_3 (Averag  (None, 5, 18, 48)        0         
	 ePooling2D)                                                     
	 conv2d_4 (Conv2D)           (None, 3, 16, 64)         27712     
	 flatten (Flatten)           (None, 3072)              0         

Try with 60,20 cropping  - total params 180k
	 input_1 (InputLayer)        [(None, 160, 320, 3)]     0         
	 cropping2d (Cropping2D)     (None, 80, 320, 3)        0         
	 rescaling (Rescaling)       (None, 80, 320, 3)        0         
	 conv2d (Conv2D)             (None, 80, 320, 6)        456       
	 average_pooling2d (AverageP  (None, 40, 160, 6)       0         
	 ooling2D)                                                       
	 conv2d_1 (Conv2D)           (None, 36, 156, 16)       2416      
	 average_pooling2d_1 (Averag  (None, 18, 78, 16)       0         
	 ePooling2D)                                                     
	 conv2d_2 (Conv2D)           (None, 16, 76, 32)        4640      
	 average_pooling2d_2 (Averag  (None, 8, 38, 32)        0         
	 ePooling2D)                                                     
	 conv2d_3 (Conv2D)           (None, 6, 36, 48)         13872     
	 average_pooling2d_3 (Averag  (None, 3, 18, 48)        0         
	 ePooling2D)                                                     
	 conv2d_4 (Conv2D)           (None, 1, 16, 64)         27712     

	Epoch 1/10
	8567/8567 [==============================] - 39s 4ms/step - loss: 0.0031 - val_loss: 0.0021
	Epoch 2/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0015 - val_loss: 0.0024
	Epoch 3/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0014 - val_loss: 0.0020
	Epoch 4/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0012 - val_loss: 0.0025
	Epoch 5/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0020
	Epoch 6/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 7/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0026
	Epoch 8/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0023
	Epoch 9/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0023
	Epoch 10/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0013 - val_loss: 0.0029

as above, but no avg pool before the last conv - 1.1M param
	Epoch 1/10
	8567/8567 [==============================] - 46s 5ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 46s 5ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 45s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 45s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 5/10
	8567/8567 [==============================] - 45s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 6/10
	8567/8567 [==============================] - 45s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 7/10
	8567/8567 [==============================] - 45s 5ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 8/10
	8567/8567 [==============================] - 45s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 9/10
	8567/8567 [==============================] - 45s 5ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 10/10
	8567/8567 [==============================] - 46s 5ms/step - loss: 0.0078 - val_loss: 0.0040

with avg pool before the last conv, drop 0.4,0.4 (instead of 0.4,0.2)
	Epoch 1/10
	8567/8567 [==============================] - 41s 5ms/step - loss: 0.0080 - val_loss: 0.0042
	Epoch 2/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0079 - val_loss: 0.0042
	Epoch 3/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 5/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 6/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 7/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0042
	Epoch 8/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 9/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 10/10
	8567/8567 [==============================] - 37s 4ms/step - loss: 0.0079 - val_loss: 0.0040

re-affirm drop 0.4,0.2
	 input_1 (InputLayer)        [(None, 160, 320, 3)]     0         
	 cropping2d (Cropping2D)     (None, 80, 320, 3)        0         
	 rescaling (Rescaling)       (None, 80, 320, 3)        0         
	 conv2d (Conv2D)             (None, 80, 320, 6)        456       
	 average_pooling2d (AverageP  (None, 40, 160, 6)       0         
	 ooling2D)                                                       
	 conv2d_1 (Conv2D)           (None, 36, 156, 16)       2416      
	 average_pooling2d_1 (Averag  (None, 18, 78, 16)       0         
	 ePooling2D)                                                     
	 conv2d_2 (Conv2D)           (None, 16, 76, 32)        4640      
	 average_pooling2d_2 (Averag  (None, 8, 38, 32)        0         
	 ePooling2D)                                                     
	 conv2d_3 (Conv2D)           (None, 6, 36, 48)         13872     
	 average_pooling2d_3 (Averag  (None, 3, 18, 48)        0         
	 ePooling2D)                                                     
	 conv2d_4 (Conv2D)           (None, 1, 16, 64)         27712     
	 flatten (Flatten)           (None, 1024)              0         
	 dropout (Dropout)           (None, 1024)              0         
	 dense (Dense)               (None, 120)               123000    
	 dropout_1 (Dropout)         (None, 120)               0         
	 dense_1 (Dense)             (None, 84)                10164     
	 dense_2 (Dense)             (None, 1)                 85   
	
	Epoch 1/10
	8567/8567 [==============================] - 40s 5ms/step - loss: 0.0036 - val_loss: 0.0020
	Epoch 2/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0016 - val_loss: 0.0021
	Epoch 3/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0014 - val_loss: 0.0018
	Epoch 4/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0013 - val_loss: 0.0025
	Epoch 5/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 6/10
	8567/8567 [==============================] - 38s 4ms/st ep - loss: 0.0012 - val_loss: 0.0022
	Epoch 7/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0022
	Epoch 8/10
	8567/8567 [==============================] - 39s 5ms/step - loss: 0.0011 - val_loss: 0.0025
	Epoch 9/10
	8567/8567 [==============================] - 39s 4ms/step - loss: 0.0011 - val_loss: 0.0021
	Epoch 10/10
	8567/8567 [==============================] - 39s 4ms/step - loss: 0.0012 - val_loss: 0.0020

drop 0.6,0.2
	Epoch 1/10
	8567/8567 [==============================] - 40s 4ms/step - loss: 0.0081 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 5/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0078 - val_loss: 0.0040
	
LeNetDrop2XConvPoorRes 
 -sth like a resnet - drop 0.4, 0.2 
 -     conv1 = layers.Conv2D(filters=6,kernel_size=(5,5),strides=(1,1)
                ,padding='same',activation='relu'
                ,name='conv1')(input)
       av1 = layers.AveragePooling2D(pool_size=(2,2),strides=(2,2)
                ,name='av1')(conv1)
       skip1 = layers.Conv2D(filters=6,kernel_size=(1,1),strides=(2,2)
                ,padding='same',activation='relu'
                ,name='skip1')(input)
       res1 = layers.add([av1,skip1]
                ,name='res1')

	Epoch 1/10
	8567/8567 [==============================] - 42s 5ms/step - loss: 0.0033 - val_loss: 0.0023
	Epoch 2/10
	8567/8567 [==============================] - 41s 5ms/step - loss: 0.0018 - val_loss: 0.0023
	Epoch 3/10
	8567/8567 [==============================] - 42s 5ms/step - loss: 0.0015 - val_loss: 0.0019
	Epoch 4/10
	8567/8567 [==============================] - 42s 5ms/step - loss: 0.0014 - val_loss: 0.0019
	Epoch 5/10
	8567/8567 [==============================] - 41s 5ms/step - loss: 0.0013 - val_loss: 0.0021
	Epoch 6/10
	8567/8567 [==============================] - 40s 5ms/step - loss: 0.0013 - val_loss: 0.0023
	Epoch 7/10
	8567/8567 [==============================] - 40s 5ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 8/10
	8567/8567 [==============================] - 40s 5ms/step - loss: 0.0011 - val_loss: 0.0029
	Epoch 9/10
	8567/8567 [==============================] - 41s 5ms/step - loss: 0.0011 - val_loss: 0.0020
	Epoch 10/10
	8567/8567 [==============================] - 41s 5ms/step - loss: 0.0011 - val_loss: 0.0023

- conv block - 191k param
    # Conv Block - half h,w double c
    conv2_1 = layers.Conv2D(filters=16,kernel_size=(5,5),strides=(1,1)
                ,padding='same',activation='relu'
                ,name='conv2_1')(av1)
    av2_1 = layers.AveragePooling2D(pool_size=(2,2) ,strides=(2,2)
                ,name='av2_1')(conv2_1)
    conv2_2 = layers.Conv2D(filters=16,kernel_size=(5,5),strides=(1,1)
                ,padding='same'
                ,name='conv2_2')(av2_1) # no activation
    skip2 = layers.Conv2D(filters=16,kernel_size=(5,5),strides=(2,2)
                ,padding='same'
                ,name='skip2')(av1) # no activation
    add2 = layers.add([conv2_2,skip2]
                ,name='add2')
    act2 = layers.Activation(keras.activations.relu,
                name='act2')(add2)

	Epoch 1/10
	8567/8567 [==============================] - 50s 6ms/step - loss: 0.0080 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 48s 6ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 48s 6ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 47s 6ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 5/10
	8567/8567 [==============================] - 47s 6ms/step - loss: 0.0078 - val_loss: 0.0040

LeNetDrop2XConvPoorRes - 5 resnet blocks
	 input_1 (InputLayer)           [(None, 160, 320, 3  0           []                               
		                        )]                                                                
	 cropping2d (Cropping2D)        (None, 80, 320, 3)   0           ['input_1[0][0]']                
	 rescaling (Rescaling)          (None, 80, 320, 3)   0           ['cropping2d[0][0]']             
	 conv1 (Conv2D)                 (None, 80, 320, 6)   456         ['rescaling[0][0]']              
	 av1 (AveragePooling2D)         (None, 40, 160, 6)   0           ['conv1[0][0]']                  

	 conv2_1 (Conv2D)               (None, 40, 160, 16)  2416        ['av1[0][0]']                    
	 av2_1 (AveragePooling2D)       (None, 20, 80, 16)   0           ['conv2_1[0][0]']                
	 conv2_2 (Conv2D)               (None, 20, 80, 16)   6416        ['av2_1[0][0]']                  
	 skip2 (Conv2D)                 (None, 20, 80, 16)   112         ['av1[0][0]']                    
	 add2 (Add)                     (None, 20, 80, 16)   0           ['conv2_2[0][0]','skip2[0][0]']                  
	 act2 (Activation)              (None, 20, 80, 16)   0           ['add2[0][0]']                   

	 conv3_1 (Conv2D)               (None, 20, 80, 16)   2320        ['act2[0][0]']                   
	 conv3_2 (Conv2D)               (None, 20, 80, 16)   2320        ['conv3_1[0][0]']                
	 skip3 (Conv2D)                 (None, 20, 80, 16)   272         ['act2[0][0]']                   
	 add3 (Add)                     (None, 20, 80, 16)   0           ['conv3_2[0][0]','skip3[0][0]']                  
	 act3 (Activation)              (None, 20, 80, 16)   0           ['add3[0][0]']                   

	 conv4_1 (Conv2D)               (None, 20, 80, 32)   4640        ['act3[0][0]']                   
	 av4_1 (AveragePooling2D)       (None, 10, 40, 32)   0           ['conv4_1[0][0]']                
	 conv4_2 (Conv2D)               (None, 10, 40, 32)   9248        ['av4_1[0][0]']                  
	 skip4 (Conv2D)                 (None, 10, 40, 32)   544         ['act3[0][0]']                   
	 add4 (Add)                     (None, 10, 40, 32)   0           ['conv4_2[0][0]','skip4[0][0]']                  
	 act4 (Activation)              (None, 10, 40, 32)   0           ['add4[0][0]']                   

	 conv5_1 (Conv2D)               (None, 10, 40, 32)   9248        ['act4[0][0]']                   
	 conv5_2 (Conv2D)               (None, 10, 40, 32)   9248        ['conv5_1[0][0]']                
	 skip5 (Conv2D)                 (None, 10, 40, 32)   1056        ['act4[0][0]']                   
	 add5 (Add)                     (None, 10, 40, 32)   0           ['conv5_2[0][0]','skip5[0][0]']                  
	 act5 (Activation)              (None, 10, 40, 32)   0           ['add5[0][0]']                   

	 conv6_1 (Conv2D)               (None, 10, 40, 48)   13872       ['act5[0][0]']                   
	 av6_1 (AveragePooling2D)       (None, 5, 20, 48)    0           ['conv6_1[0][0]']                
	 conv6_2 (Conv2D)               (None, 5, 20, 48)    20784       ['av6_1[0][0]']                  
	 skip6 (Conv2D)                 (None, 5, 20, 48)    1584        ['act5[0][0]']                   
	 add6 (Add)                     (None, 5, 20, 48)    0           ['conv6_2[0][0]','skip6[0][0]']                  
	 act6 (Activation)              (None, 5, 20, 48)    0           ['add6[0][0]']                   
		                                                                                          
	 conv2d (Conv2D)                (None, 3, 18, 64)    27712       ['act6[0][0]']                   
	 flatten (Flatten)              (None, 3456)         0           ['conv2d[0][0]']                 
	 dropout (Dropout)              (None, 3456)         0           ['flatten[0][0]']                
	 dense (Dense)                  (None, 120)          414840      ['dropout[0][0]']                
	 dropout_1 (Dropout)            (None, 120)          0           ['dense[0][0]']                  
	 dense_1 (Dense)                (None, 84)           10164       ['dropout_1[0][0]']              
	 dense_2 (Dense)                (None, 1)            85          ['dense_1[0][0]']                

	Epoch 1/10
	8567/8567 [==============================] - 74s 8ms/step - loss: 0.0080 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 72s 8ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 72s 8ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 5/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 6/10
	8567/8567 [==============================] - 72s 8ms/step - loss: 0.0078 - val_loss: 0.0040


as above with drop 0.2,0.2 (instead of 0.4,0.2)
	8567/8567 [==============================] - 74s 8ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0078 - val_loss: 0.0041


drop 0.4,0.2 and stride 2 in the last conv2d
	Epoch 1/10
	8567/8567 [==============================] - 72s 8ms/step - loss: 0.0080 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 70s 8ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 69s 8ms/step - loss: 0.0078 - val_loss: 0.0040

stride 1 and another average layer
	Epoch 1/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 69s 8ms/step - loss: 0.0078 - val_loss: 0.0041

lr 0.0005 (instead of 0.001)
	Epoch 1/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 68s 8ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 5/10

8 (instea of 6) filters in conv1 (still LeNetDrop2XConvPoorRes with 5 res blocks)
	Epoch 1/10
	8567/8567 [==============================] - 73s 8ms/step - loss: 0.0079 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 70s 8ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0079 - val_loss: 0.0040
	Epoch 5/10
	8567/8567 [==============================] - 70s 8ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 6/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 7/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 8/10
	8567/8567 [==============================] - 71s 8ms/step - loss: 0.0078 - val_loss: 0.0041

back to LeNetDrop2XConv
 - with lr 0.0005 (instead of 0.001)

	8567/8567 [==============================] - 40s 5ms/step - loss: 0.0035 - val_loss: 0.0023
	Epoch 2/10
	8567/8567 [==============================] - 39s 5ms/step - loss: 0.0017 - val_loss: 0.0019
	Epoch 3/10
	8567/8567 [==============================] - 39s 5ms/step - loss: 0.0015 - val_loss: 0.0020
	Epoch 4/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0013 - val_loss: 0.0018
	Epoch 5/10
	8567/8567 [==============================] - 40s 5ms/step - loss: 0.0016 - val_loss: 0.0018
	Epoch 6/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0018
	Epoch 7/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0020
	Epoch 8/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 9/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 10/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0020

decay rate 0.9 at 10000 (instead of 0.96 and 20000
	Epoch 1/10
	8567/8567 [==============================] - 40s 4ms/step - loss: 0.0080 - val_loss: 0.0041
	Epoch 2/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0078 - val_loss: 0.0041
	Epoch 3/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0078 - val_loss: 0.0040
	Epoch 4/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0079 - val_loss: 0.0040

lr 0.01 (and 0.96 at 20000)
	Epoch 1/10
	8567/8567 [==============================] - 40s 4ms/step - loss: 0.0042 - val_loss: 0.0022
	Epoch 2/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0016 - val_loss: 0.0020
	Epoch 3/10
	8567/8567 [==============================] - 39s 5ms/step - loss: 0.0014 - val_loss: 0.0018
	Epoch 4/10
	8567/8567 [==============================] - 39s 5ms/step - loss: 0.0013 - val_loss: 0.0021
	Epoch 5/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0013 - val_loss: 0.0019
	Epoch 6/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0019
	Epoch 7/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0023
	Epoch 8/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0023
	Epoch 9/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0018
	Epoch 10/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0025

lr 0.1 and decay 0.5 at 10000
	Epoch 1/10
	8567/8567 [==============================] - 39s 4ms/step - loss: 0.0032 - val_loss: 0.0018
	Epoch 2/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0015 - val_loss: 0.0022
	Epoch 3/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0014 - val_loss: 0.0020
	Epoch 4/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 5/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0022
	Epoch 6/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0021
	Epoch 7/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0026
	Epoch 8/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0011 - val_loss: 0.0020
	Epoch 9/10
	8567/8567 [==============================] - 38s 4ms/step - loss: 0.0012 - val_loss: 0.0018
	Epoch 10/10
	8567/8567 [==============================] - 39s 5ms/step - loss: 0.0011 - val_loss: 0.0021

LeNetDrop2XConv
lr 0.1, decay 0.5 at 10000 ,batch size 4
	Epoch 1/10
	2142/2142 [==============================] - 26s 11ms/step - loss: 0.0023 - val_loss: 0.0019
	Epoch 2/10
	2142/2142 [==============================] - 23s 11ms/step - loss: 0.0010 - val_loss: 0.0019
	Epoch 3/10
	2142/2142 [==============================] - 23s 11ms/step - loss: 9.4425e-04 - val_loss: 0.0019
	Epoch 4/10
	2142/2142 [==============================] - 23s 11ms/step - loss: 8.5054e-04 - val_loss: 0.0023
	Epoch 5/10
	2142/2142 [==============================] - 23s 11ms/step - loss: 8.1509e-04 - val_loss: 0.0019
	Epoch 6/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 7.8604e-04 - val_loss: 0.0018
	Epoch 7/10
	2142/2142 [==============================] - 23s 11ms/step - loss: 7.5249e-04 - val_loss: 0.0025
	Epoch 8/10
	2142/2142 [==============================] - 23s 11ms/step - loss: 7.8333e-04 - val_loss: 0.0022
	Epoch 9/10
	2142/2142 [==============================] - 23s 11ms/step - loss: 6.9262e-04 - val_loss: 0.0022
	Epoch 10/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 7.0678e-04 - val_loss: 0.0021

LeNetDrop2XConv 
 - anotehr drop 0.2  before output
 - split with sklearn
	Epoch 1/10
	2142/2142 [==============================] - 26s 11ms/step - loss: 0.0024 - val_loss: 0.0012
	Epoch 2/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 0.0013 - val_loss: 9.6605e-04
	Epoch 3/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 0.0013 - val_loss: 9.1743e-04
	Epoch 4/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 0.0013 - val_loss: 8.8135e-04
	Epoch 5/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 0.0012 - val_loss: 7.4072e-04
	Epoch 6/10
	2142/2142 [==============================] - 25s 11ms/step - loss: 0.0011 - val_loss: 6.8074e-04
	Epoch 7/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 0.0010 - val_loss: 9.3968e-04
	Epoch 8/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 0.0011 - val_loss: 6.7791e-04
	Epoch 9/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 0.0011 - val_loss: 0.0011
	Epoch 10/10
	2142/2142 [==============================] - 24s 11ms/step - loss: 9.9898e-04 - val_loss: 6.4730e-04

